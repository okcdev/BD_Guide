## 说明

以微博为例，数据库中存储着大量的用户微博信息，其格式为：

|微博Id	|微博内容|
|---|---|
|223	|滴滴打车超级快的|
|224	|站在路边等了三小时出租车都没有|
|…	|…|

如果某o2o打车平台交了一笔广告费，需要微博帮其将广告推送给可能有需要的人，如何在大量的微博中找出对打车有需求的人呢？
具体过程大概如下（以打车广告为例）：

> 1.对每条微博进行分词，得到该微博的所有词条
> 2.使用公式W=TF+log(N/DF)计算这条微博中，打车这个词的权重
> 3.推送打车广告的时候选择打车这个词权重比较高的微博用户进行推送

### 公式解释

**W=TF+log(N/DF)**这个公式是一个验证过的数学公式：

> * W：表示该词的权重
> * TF：表示该词在该微博中出现的次数
> * N：所有微博条数
> * DF：所有微博中出现了该词的条数

W(权重)=TF(该词在该微博中出现的次数)很好理解，因为一条微博中出现打车这个词的**次数越高，该用户很有可能对打车的关注度就越高**

N(所有微博条数)/DF(所有微博中出现了该词的条数)的作用是为该词添加一个附加权重，例如，一共有5条微博，只有一条出现了打车这个词，5/1=5，而如果5条都出现了打车，5/5=1

**很明显，如果所有人当中只有你提及到了打车，那肯定是你的权重最高**，log(N/DF)取了一个自然对数以防N/DF的值太大

所以整个流程中最重要的就是**根据公式进行计算**

### 注意

具体实现的时候不能只计算打车这个词在每条微博中的权重，而是要**计算所有微博中所有词组在该微博中的权重**

为什么呢？   
这次推送的是打车广告，计算打车的权重，如果又来了个电影广告，难道还要重新计算一次电影的权重吗   
**一次将所有词的权重计算出来，根据需要提取即可**

### MapReduce过程

为了根据该公式进行计算W，我们需要得到TF,N和DF的值

分词可以参考[MapReduceDemo](https://github.com/chubbyjiang/MapReduce)中**第五部分庖丁分词器的使用**

### 计算TF和N

根据MapReduce的尿性，输入一行微博数据，对微博内容进行分词，得到许多需要计算权重的词   
这里我们需要统计的是**该词在该微博中出现的次数**，一个类似WordCount的程序

mapper的输出value可以确定为1，key如果是词本身的话，那么到reduce中计算的将会是所有微博中该词出现的次数，所以key除了词还要**带上一个唯一的微博Id标识**   
这样一来，相同微博中的相同词会被送到相同的reducer中

在mapper做分词和输出的时候，也可以顺便将N计算出来，每次输入都是一条微博，把key固定，value为1，reducer中直接累加就可以得到N的值

### 计算DF

要直接从原始的微博数据中计算每个词的DF有些复杂，但是我们可以在TF的输出结果上计算DF
TF的结果类似：   
key:打车_微博Id1,value:5

注意到这个结果的一个特点：结果集中，key中有打车这个词的，**就是其所在的微博出现了打车这个关键词，计数1，最后累加结果即可**

### 计算公式

为了计算改公式，我们需要把TF,N和DF的值收集起来，最合适的方法就是**使用DistributedCache**   
使用方式可以参考：[MapReduce中的DistributedCache](http://www.xiaohei.info/2016/02/26/mapreduce-distributed-cache/)

将N、DF的值从DistributedCache中取出来，存入两个Map集合中，读取TF结果文件对每个词进行计算输出即可得到结果

## 代码

[源码地址](https://github.com/chubbyjiang/MapReduce)第10部分(weiboad)